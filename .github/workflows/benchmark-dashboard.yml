name: Benchmark Dashboard

on:
  workflow_dispatch:
    inputs:
      provider:
        description: "Provider (ollama, copilot, gemini)"
        required: true
        type: choice
        options:
          - ollama
          - copilot
          - gemini
      model:
        description: "Model name (e.g., qwen-coder-next:cloud)"
        required: true
        type: string
      skill:
        description: "Optional specific skill to test"
        required: false
        type: string

jobs:
  run-benchmark-and-publish:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          ref: main
          path: repo

      - uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: Run benchmark
        working-directory: repo
        env:
          OLLAMA_API_KEY: ${{ secrets.OLLAMA_API_KEY }}
        run: |
          uv sync --project tests
          uv run --project tests tests/evaluator.py \
            --provider ${{ inputs.provider }} \
            --model ${{ inputs.model }} \
            --judge \
            --verbose \
            --report

      - name: Publish to benchmark-history branch
        working-directory: repo
        run: |
          # Run publish_benchmarks.py with correct paths
          uv run --project tests python3 ci/publish_benchmarks.py \
            --provider ${{ inputs.provider }} \
            --model ${{ inputs.model }} \
            --branch benchmark-history \
            --no-benchmark \
            --results-dir "tests/results" \
            --output-dir "docs/benchmarks"
